{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f8af3e37-e086-45ad-8529-f0a0a871a552",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sb3_contrib import QRDQN\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "\n",
    "from envs.bess_env import BatteryEnv\n",
    "from utils.csv_handler import load_price_data, save_records\n",
    "from utils.eval_handler import evaluate_rollout\n",
    "from utils.forecast_scenario import ForecastScenarioGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "049b8f3c-01e5-40a5-a55c-a37215f1e5c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------\n",
    "# Experiment config\n",
    "# --------------------------------------------------\n",
    "training_steps_list = [10_000, 20_000, 40_000, 60_000, 80_000, 100_000]\n",
    "n_runs = 5\n",
    "base_seed = 10\n",
    "\n",
    "forecast_horizon_hours = 3.0\n",
    "dt_hours = 0.25\n",
    "H = int(round(forecast_horizon_hours / dt_hours))\n",
    "\n",
    "# --------------------------------------------------\n",
    "# Load data\n",
    "# --------------------------------------------------\n",
    "dfp_train, price_train, ts_train = load_price_data(\n",
    "    csv_path=\"../../../data/electricity_price/dayahead_2024_11.csv\",\n",
    "    resolution=\"15min\",\n",
    ")\n",
    "\n",
    "dfp_eval, price_eval, ts_eval = load_price_data(\n",
    "    csv_path=\"../../../data/electricity_price/dayahead_2025_11.csv\",\n",
    "    resolution=\"15min\",\n",
    "    time_range=(\"2025-11-01\", \"2025-11-07\"),\n",
    ")\n",
    "\n",
    "# --------------------------------------------------\n",
    "# Forecast scenarios (fixed across runs)\n",
    "# --------------------------------------------------\n",
    "price_scenario_gen = ForecastScenarioGenerator(\n",
    "    horizon_steps=H,\n",
    "    sigma0=0.01,\n",
    "    sigmaH=0.06,\n",
    "    schedule=\"sqrt\",\n",
    "    base_seed=1234,\n",
    ")\n",
    "\n",
    "# --------------------------------------------------\n",
    "# Storage for results\n",
    "# --------------------------------------------------\n",
    "records = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "86dc62ff-256c-4445-bd26-48596a270f1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Training steps: 10000 ===\n",
      "  Run 1/5 (seed=10)\n",
      "Episode finished after 672 steps\n",
      "    → Run reward = 0.75\n",
      "  Run 2/5 (seed=11)\n",
      "Episode finished after 672 steps\n",
      "    → Run reward = 0.79\n",
      "  Run 3/5 (seed=12)\n",
      "Episode finished after 672 steps\n",
      "    → Run reward = 0.74\n",
      "  Run 4/5 (seed=13)\n",
      "Episode finished after 672 steps\n",
      "    → Run reward = -0.61\n",
      "  Run 5/5 (seed=14)\n",
      "Episode finished after 672 steps\n",
      "    → Run reward = 0.62\n",
      "  → mean reward = 0.46 ± 0.54\n",
      "\n",
      "=== Training steps: 20000 ===\n",
      "  Run 1/5 (seed=10)\n",
      "Episode finished after 672 steps\n",
      "    → Run reward = 1.79\n",
      "  Run 2/5 (seed=11)\n",
      "Episode finished after 672 steps\n",
      "    → Run reward = 4.26\n",
      "  Run 3/5 (seed=12)\n",
      "Episode finished after 672 steps\n",
      "    → Run reward = 1.04\n",
      "  Run 4/5 (seed=13)\n",
      "Episode finished after 672 steps\n",
      "    → Run reward = 3.73\n",
      "  Run 5/5 (seed=14)\n",
      "Episode finished after 672 steps\n",
      "    → Run reward = 2.80\n",
      "  → mean reward = 2.73 ± 1.19\n",
      "\n",
      "=== Training steps: 40000 ===\n",
      "  Run 1/5 (seed=10)\n",
      "Episode finished after 672 steps\n",
      "    → Run reward = 2.86\n",
      "  Run 2/5 (seed=11)\n",
      "Episode finished after 672 steps\n",
      "    → Run reward = 2.76\n",
      "  Run 3/5 (seed=12)\n",
      "Episode finished after 672 steps\n",
      "    → Run reward = 1.82\n",
      "  Run 4/5 (seed=13)\n",
      "Episode finished after 672 steps\n",
      "    → Run reward = 8.93\n",
      "  Run 5/5 (seed=14)\n",
      "Episode finished after 672 steps\n",
      "    → Run reward = 12.72\n",
      "  → mean reward = 5.82 ± 4.27\n",
      "\n",
      "=== Training steps: 60000 ===\n",
      "  Run 1/5 (seed=10)\n",
      "Episode finished after 672 steps\n",
      "    → Run reward = 10.69\n",
      "  Run 2/5 (seed=11)\n",
      "Episode finished after 672 steps\n",
      "    → Run reward = 11.53\n",
      "  Run 3/5 (seed=12)\n",
      "Episode finished after 672 steps\n",
      "    → Run reward = 7.10\n",
      "  Run 4/5 (seed=13)\n",
      "Episode finished after 672 steps\n",
      "    → Run reward = 3.37\n",
      "  Run 5/5 (seed=14)\n",
      "Episode finished after 672 steps\n",
      "    → Run reward = 12.12\n",
      "  → mean reward = 8.96 ± 3.29\n",
      "\n",
      "=== Training steps: 80000 ===\n",
      "  Run 1/5 (seed=10)\n",
      "Episode finished after 672 steps\n",
      "    → Run reward = 11.62\n",
      "  Run 2/5 (seed=11)\n",
      "Episode finished after 672 steps\n",
      "    → Run reward = 4.65\n",
      "  Run 3/5 (seed=12)\n",
      "Episode finished after 672 steps\n",
      "    → Run reward = 11.25\n",
      "  Run 4/5 (seed=13)\n",
      "Episode finished after 672 steps\n",
      "    → Run reward = 10.16\n",
      "  Run 5/5 (seed=14)\n",
      "Episode finished after 672 steps\n",
      "    → Run reward = 6.74\n",
      "  → mean reward = 8.89 ± 2.73\n",
      "\n",
      "=== Training steps: 100000 ===\n",
      "  Run 1/5 (seed=10)\n",
      "Episode finished after 672 steps\n",
      "    → Run reward = 12.32\n",
      "  Run 2/5 (seed=11)\n",
      "Episode finished after 672 steps\n",
      "    → Run reward = 11.76\n",
      "  Run 3/5 (seed=12)\n",
      "Episode finished after 672 steps\n",
      "    → Run reward = 1.23\n",
      "  Run 4/5 (seed=13)\n",
      "Episode finished after 672 steps\n",
      "    → Run reward = 11.75\n",
      "  Run 5/5 (seed=14)\n",
      "Episode finished after 672 steps\n",
      "    → Run reward = 10.69\n",
      "  → mean reward = 9.55 ± 4.19\n",
      "\n",
      "=== Summary ===\n",
      "   training_steps      mean       std\n",
      "0           10000  0.458119  0.599405\n",
      "1           20000  2.725316  1.330598\n",
      "2           40000  5.818152  4.779447\n",
      "3           60000  8.961650  3.682517\n",
      "4           80000  8.886265  3.051379\n",
      "5          100000  9.549876  4.686643\n"
     ]
    }
   ],
   "source": [
    "# ==================================================\n",
    "# Main experiment loop\n",
    "# ==================================================\n",
    "for total_steps in training_steps_list:\n",
    "    print(f\"\\n=== Training steps: {total_steps} ===\")\n",
    "\n",
    "    run_rewards = []\n",
    "\n",
    "    for run_id in range(n_runs):\n",
    "        seed = base_seed + run_id\n",
    "        print(f\"  Run {run_id+1}/{n_runs} (seed={seed})\")\n",
    "\n",
    "        # -----------------------------\n",
    "        # Training env\n",
    "        # -----------------------------\n",
    "        train_env_raw = BatteryEnv(\n",
    "            price_series=price_train,\n",
    "            timestamps=ts_train,\n",
    "            dt_hours=dt_hours,\n",
    "            capacity_kWh=50.0,\n",
    "            p_max_kW=10.0,\n",
    "            use_discrete_actions=True,\n",
    "            use_price_forecast=True,\n",
    "            forecast_horizon_hours=forecast_horizon_hours,\n",
    "            episode_days=7.0,\n",
    "            random_start=True,\n",
    "            random_seed=seed,\n",
    "            price_scenario_gen=price_scenario_gen,\n",
    "        )\n",
    "\n",
    "        train_env = Monitor(train_env_raw)\n",
    "\n",
    "        policy_kwargs = dict(n_quantiles=100)\n",
    "        model = QRDQN(\n",
    "            \"MlpPolicy\",\n",
    "            train_env,\n",
    "            policy_kwargs=policy_kwargs,\n",
    "            learning_rate=1e-3,\n",
    "            buffer_size=50_000,\n",
    "            learning_starts=1_000,\n",
    "            batch_size=64,\n",
    "            gamma=0.99,\n",
    "            train_freq=4,\n",
    "            target_update_interval=1_000,\n",
    "            exploration_initial_eps=1.0,\n",
    "            exploration_final_eps=0.05,\n",
    "            exploration_fraction=0.3,\n",
    "            verbose=0,\n",
    "            seed=seed,\n",
    "        )\n",
    "\n",
    "        model.learn(total_timesteps=total_steps)\n",
    "\n",
    "        # -----------------------------\n",
    "        # Evaluation env (fixed)\n",
    "        # -----------------------------\n",
    "        eval_env = BatteryEnv(\n",
    "            price_series=price_eval,\n",
    "            timestamps=ts_eval,\n",
    "            dt_hours=dt_hours,\n",
    "            capacity_kWh=50.0,\n",
    "            p_max_kW=10.0,\n",
    "            use_discrete_actions=True,\n",
    "            use_price_forecast=True,\n",
    "            forecast_horizon_hours=forecast_horizon_hours,\n",
    "            episode_days=7.0,\n",
    "            random_start=False,\n",
    "            random_seed=999,\n",
    "            price_scenario_gen=price_scenario_gen,\n",
    "            scenario_id=0,\n",
    "            vary_scenario_per_episode=False,\n",
    "        )\n",
    "\n",
    "        rollout = evaluate_rollout(model=model, env=eval_env)\n",
    "\n",
    "        total_reward = np.sum(rollout[\"reward\"])\n",
    "        run_rewards.append(total_reward)\n",
    "\n",
    "        print(f\"    → Run reward = {total_reward:.2f}\")\n",
    "\n",
    "        records.append({\n",
    "            \"agent\": \"QR-DQN\",\n",
    "            \"training_steps\": total_steps,\n",
    "            \"run_id\": run_id,\n",
    "            \"seed\": seed,\n",
    "            \"total_reward\": total_reward,\n",
    "        })\n",
    "\n",
    "    # -----------------------------\n",
    "    # Summary for this step size\n",
    "    # -----------------------------\n",
    "    mean_r = np.mean(run_rewards)\n",
    "    std_r = np.std(run_rewards)\n",
    "\n",
    "    print(f\"  → mean reward = {mean_r:.2f} ± {std_r:.2f}\")\n",
    "\n",
    "# --------------------------------------------------\n",
    "# Convert to DataFrame\n",
    "# --------------------------------------------------\n",
    "df_results = pd.DataFrame(records)\n",
    "\n",
    "summary = (\n",
    "    df_results\n",
    "    .groupby(\"training_steps\")[\"total_reward\"]\n",
    "    .agg([\"mean\", \"std\"])\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "print(\"\\n=== Summary ===\")\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "09b658bf-8456-47bb-aace-e5ff716e00e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[save_experiment_records] Saved 30 new rows (total=60) to results/learning_steps_records.csv\n"
     ]
    }
   ],
   "source": [
    "save_records(\n",
    "    records=records,\n",
    "    out_path=\"results/learning_steps_records.csv\",\n",
    "    experiment_id=\"qrdqn_learning_steps_v1\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73cb0041-0528-4d3c-a9c4-9b7938b89b41",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (drl_bess)",
   "language": "python",
   "name": "drl_bess"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
