{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "37814f1a-fd6a-440b-89fb-f9219bbe76c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from stable_baselines3 import TD3\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.noise import NormalActionNoise\n",
    "\n",
    "from envs.bess_env import BatteryEnv\n",
    "from utils.csv_handler import load_price_data, save_records\n",
    "from utils.eval_handler import evaluate_rollout\n",
    "from utils.forecast_scenario import ForecastScenarioGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c4623061-172d-4d6f-b3f9-5eb468e222a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------\n",
    "# Experiment config\n",
    "# --------------------------------------------------\n",
    "training_steps_list = [10_000, 20_000, 40_000, 60_000, 80_000, 100_000]\n",
    "n_runs = 5\n",
    "base_seed = 10\n",
    "\n",
    "forecast_horizon_hours = 3.0\n",
    "dt_hours = 0.25\n",
    "H = int(round(forecast_horizon_hours / dt_hours))\n",
    "\n",
    "# --------------------------------------------------\n",
    "# Load data\n",
    "# --------------------------------------------------\n",
    "dfp_train, price_train, ts_train = load_price_data(\n",
    "    csv_path=\"../../../data/electricity_price/dayahead_2024_11.csv\",\n",
    "    resolution=\"15min\",\n",
    ")\n",
    "\n",
    "dfp_eval, price_eval, ts_eval = load_price_data(\n",
    "    csv_path=\"../../../data/electricity_price/dayahead_2025_11.csv\",\n",
    "    resolution=\"15min\",\n",
    "    time_range=(\"2025-11-01\", \"2025-11-07\"),\n",
    ")\n",
    "\n",
    "# --------------------------------------------------\n",
    "# Forecast scenarios (fixed across runs)\n",
    "# --------------------------------------------------\n",
    "price_scenario_gen = ForecastScenarioGenerator(\n",
    "    horizon_steps=H,\n",
    "    sigma0=0.01,\n",
    "    sigmaH=0.06,\n",
    "    schedule=\"sqrt\",\n",
    "    base_seed=1234,\n",
    ")\n",
    "\n",
    "# --------------------------------------------------\n",
    "# Storage for results\n",
    "# --------------------------------------------------\n",
    "records = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1d797282-9448-49a4-acd9-c9f776244a37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Training steps: 10000 ===\n",
      "  Run 1/5 (seed=10)\n",
      "Episode finished after 672 steps\n",
      "    → Run reward = 2.48\n",
      "  Run 2/5 (seed=11)\n",
      "Episode finished after 672 steps\n",
      "    → Run reward = 2.46\n",
      "  Run 3/5 (seed=12)\n",
      "Episode finished after 672 steps\n",
      "    → Run reward = 0.53\n",
      "  Run 4/5 (seed=13)\n",
      "Episode finished after 672 steps\n",
      "    → Run reward = 6.01\n",
      "  Run 5/5 (seed=14)\n",
      "Episode finished after 672 steps\n",
      "    → Run reward = 6.75\n",
      "  → mean reward = 3.65 ± 2.35\n",
      "\n",
      "=== Training steps: 20000 ===\n",
      "  Run 1/5 (seed=10)\n",
      "Episode finished after 672 steps\n",
      "    → Run reward = 7.13\n",
      "  Run 2/5 (seed=11)\n",
      "Episode finished after 672 steps\n",
      "    → Run reward = 1.18\n",
      "  Run 3/5 (seed=12)\n",
      "Episode finished after 672 steps\n",
      "    → Run reward = 0.01\n",
      "  Run 4/5 (seed=13)\n",
      "Episode finished after 672 steps\n",
      "    → Run reward = 15.24\n",
      "  Run 5/5 (seed=14)\n",
      "Episode finished after 672 steps\n",
      "    → Run reward = 10.02\n",
      "  → mean reward = 6.71 ± 5.65\n",
      "\n",
      "=== Training steps: 40000 ===\n",
      "  Run 1/5 (seed=10)\n",
      "Episode finished after 672 steps\n",
      "    → Run reward = 1.52\n",
      "  Run 2/5 (seed=11)\n",
      "Episode finished after 672 steps\n",
      "    → Run reward = 11.67\n",
      "  Run 3/5 (seed=12)\n",
      "Episode finished after 672 steps\n",
      "    → Run reward = 0.71\n",
      "  Run 4/5 (seed=13)\n",
      "Episode finished after 672 steps\n",
      "    → Run reward = 9.91\n",
      "  Run 5/5 (seed=14)\n",
      "Episode finished after 672 steps\n",
      "    → Run reward = 3.80\n",
      "  → mean reward = 5.52 ± 4.45\n",
      "\n",
      "=== Training steps: 60000 ===\n",
      "  Run 1/5 (seed=10)\n",
      "Episode finished after 672 steps\n",
      "    → Run reward = -0.18\n",
      "  Run 2/5 (seed=11)\n",
      "Episode finished after 672 steps\n",
      "    → Run reward = 13.41\n",
      "  Run 3/5 (seed=12)\n",
      "Episode finished after 672 steps\n",
      "    → Run reward = -0.77\n",
      "  Run 4/5 (seed=13)\n",
      "Episode finished after 672 steps\n",
      "    → Run reward = 6.94\n",
      "  Run 5/5 (seed=14)\n",
      "Episode finished after 672 steps\n",
      "    → Run reward = 10.35\n",
      "  → mean reward = 5.95 ± 5.63\n",
      "\n",
      "=== Training steps: 80000 ===\n",
      "  Run 1/5 (seed=10)\n",
      "Episode finished after 672 steps\n",
      "    → Run reward = 5.43\n",
      "  Run 2/5 (seed=11)\n",
      "Episode finished after 672 steps\n",
      "    → Run reward = 14.32\n",
      "  Run 3/5 (seed=12)\n",
      "Episode finished after 672 steps\n",
      "    → Run reward = 2.39\n",
      "  Run 4/5 (seed=13)\n",
      "Episode finished after 672 steps\n",
      "    → Run reward = 7.21\n",
      "  Run 5/5 (seed=14)\n",
      "Episode finished after 672 steps\n",
      "    → Run reward = 3.78\n",
      "  → mean reward = 6.63 ± 4.17\n",
      "\n",
      "=== Training steps: 100000 ===\n",
      "  Run 1/5 (seed=10)\n",
      "Episode finished after 672 steps\n",
      "    → Run reward = 3.13\n",
      "  Run 2/5 (seed=11)\n",
      "Episode finished after 672 steps\n",
      "    → Run reward = 6.09\n",
      "  Run 3/5 (seed=12)\n",
      "Episode finished after 672 steps\n",
      "    → Run reward = 2.39\n",
      "  Run 4/5 (seed=13)\n",
      "Episode finished after 672 steps\n",
      "    → Run reward = 7.49\n",
      "  Run 5/5 (seed=14)\n",
      "Episode finished after 672 steps\n",
      "    → Run reward = 3.76\n",
      "  → mean reward = 4.57 ± 1.91\n",
      "\n",
      "=== Summary ===\n",
      "   training_steps      mean       std\n",
      "0           10000  3.646984  2.630258\n",
      "1           20000  6.714364  6.311450\n",
      "2           40000  5.523465  4.980788\n",
      "3           60000  5.949760  6.296603\n",
      "4           80000  6.625820  4.665431\n",
      "5          100000  4.574548  2.139492\n"
     ]
    }
   ],
   "source": [
    "# ==================================================\n",
    "# Main experiment loop\n",
    "# ==================================================\n",
    "for total_steps in training_steps_list:\n",
    "    print(f\"\\n=== Training steps: {total_steps} ===\")\n",
    "\n",
    "    run_rewards = []\n",
    "\n",
    "    for run_id in range(n_runs):\n",
    "        seed = base_seed + run_id\n",
    "        print(f\"  Run {run_id+1}/{n_runs} (seed={seed})\")\n",
    "\n",
    "        # -----------------------------\n",
    "        # Training env\n",
    "        # -----------------------------\n",
    "        train_env_raw = BatteryEnv(\n",
    "            price_series=price_train,\n",
    "            timestamps=ts_train,\n",
    "            dt_hours=dt_hours,\n",
    "            capacity_kWh=50.0,\n",
    "            p_max_kW=10.0,\n",
    "            use_discrete_actions=False,\n",
    "            use_price_forecast=True,\n",
    "            forecast_horizon_hours=forecast_horizon_hours,\n",
    "            episode_days=7.0,\n",
    "            random_start=True,\n",
    "            random_seed=seed,\n",
    "            price_scenario_gen=price_scenario_gen,\n",
    "        )\n",
    "\n",
    "        train_env = Monitor(train_env_raw)\n",
    "\n",
    "        # Action noise for TD3 (needed for exploration)\n",
    "        n_actions = train_env.action_space.shape[-1]   # should be 1\n",
    "        action_noise = NormalActionNoise(\n",
    "            mean=np.zeros(n_actions),\n",
    "            sigma=0.1 * np.ones(n_actions),            # exploration strength\n",
    "        )\n",
    "              \n",
    "        model = TD3(\n",
    "            \"MlpPolicy\",\n",
    "            train_env,\n",
    "            action_noise=action_noise,\n",
    "            learning_rate=1e-3,\n",
    "            buffer_size=100_000,\n",
    "            learning_starts=1_000,\n",
    "            batch_size=256,\n",
    "            tau=0.005,\n",
    "            gamma=0.99,\n",
    "            train_freq=(1, \"step\"),\n",
    "            gradient_steps=1,\n",
    "            policy_delay=2,\n",
    "            verbose=0,\n",
    "            seed=seed,\n",
    "        )\n",
    "\n",
    "        model.learn(total_timesteps=total_steps)\n",
    "\n",
    "        # -----------------------------\n",
    "        # Evaluation env (fixed)\n",
    "        # -----------------------------\n",
    "        eval_env = BatteryEnv(\n",
    "            price_series=price_eval,\n",
    "            timestamps=ts_eval,\n",
    "            dt_hours=dt_hours,\n",
    "            capacity_kWh=50.0,\n",
    "            p_max_kW=10.0,\n",
    "            use_discrete_actions=False,\n",
    "            use_price_forecast=True,\n",
    "            forecast_horizon_hours=forecast_horizon_hours,\n",
    "            episode_days=7.0,\n",
    "            random_start=False,\n",
    "            random_seed=999,\n",
    "            price_scenario_gen=price_scenario_gen,\n",
    "            scenario_id=0,\n",
    "            vary_scenario_per_episode=False,\n",
    "        )\n",
    "\n",
    "        rollout = evaluate_rollout(model=model, env=eval_env)\n",
    "\n",
    "        total_reward = np.sum(rollout[\"reward\"])\n",
    "        run_rewards.append(total_reward)\n",
    "\n",
    "        print(f\"    → Run reward = {total_reward:.2f}\")\n",
    "\n",
    "        records.append({\n",
    "            \"agent\": \"TD3\",\n",
    "            \"training_steps\": total_steps,\n",
    "            \"run_id\": run_id,\n",
    "            \"seed\": seed,\n",
    "            \"total_reward\": total_reward,\n",
    "        })\n",
    "\n",
    "    # -----------------------------\n",
    "    # Summary for this step size\n",
    "    # -----------------------------\n",
    "    mean_r = np.mean(run_rewards)\n",
    "    std_r = np.std(run_rewards)\n",
    "\n",
    "    print(f\"  → mean reward = {mean_r:.2f} ± {std_r:.2f}\")\n",
    "\n",
    "# --------------------------------------------------\n",
    "# Convert to DataFrame\n",
    "# --------------------------------------------------\n",
    "df_results = pd.DataFrame(records)\n",
    "\n",
    "summary = (\n",
    "    df_results\n",
    "    .groupby(\"training_steps\")[\"total_reward\"]\n",
    "    .agg([\"mean\", \"std\"])\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "print(\"\\n=== Summary ===\")\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "be532108-86b6-4674-9aaf-84c09e44ffc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[save_experiment_records] Saved 30 new rows (total=30) to results/learning_steps_records.csv\n"
     ]
    }
   ],
   "source": [
    "save_records(\n",
    "    records=records,\n",
    "    out_path=\"results/learning_steps_records.csv\",\n",
    "    experiment_id=\"td3_learning_steps_v1\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "654d9aa3-6003-4020-8ff7-e6c15e9ba77e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (drl_bess)",
   "language": "python",
   "name": "drl_bess"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
