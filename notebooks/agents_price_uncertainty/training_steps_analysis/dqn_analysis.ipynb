{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "821ff6fc-deaf-42f0-8d83-14262198be2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from stable_baselines3 import DQN\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "\n",
    "from envs.bess_env import BatteryEnv\n",
    "from utils.csv_handler import load_price_data, save_records\n",
    "from utils.eval_handler import evaluate_rollout\n",
    "from utils.forecast_scenario import ForecastScenarioGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e10cec68-ff2f-4bc0-9554-c9cbce2d26d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------\n",
    "# Experiment config\n",
    "# --------------------------------------------------\n",
    "training_steps_list = [10_000, 20_000, 40_000, 60_000, 80_000, 100_000]\n",
    "n_runs = 5\n",
    "base_seed = 10\n",
    "\n",
    "forecast_horizon_hours = 3.0\n",
    "dt_hours = 0.25\n",
    "H = int(round(forecast_horizon_hours / dt_hours))\n",
    "\n",
    "# --------------------------------------------------\n",
    "# Load data\n",
    "# --------------------------------------------------\n",
    "dfp_train, price_train, ts_train = load_price_data(\n",
    "    csv_path=\"../../../data/electricity_price/dayahead_2024_11.csv\",\n",
    "    resolution=\"15min\",\n",
    ")\n",
    "\n",
    "dfp_eval, price_eval, ts_eval = load_price_data(\n",
    "    csv_path=\"../../../data/electricity_price/dayahead_2025_11.csv\",\n",
    "    resolution=\"15min\",\n",
    "    time_range=(\"2025-11-01\", \"2025-11-07\"),\n",
    ")\n",
    "\n",
    "# --------------------------------------------------\n",
    "# Forecast scenarios (fixed across runs)\n",
    "# --------------------------------------------------\n",
    "price_scenario_gen = ForecastScenarioGenerator(\n",
    "    horizon_steps=H,\n",
    "    sigma0=0.01,\n",
    "    sigmaH=0.06,\n",
    "    schedule=\"sqrt\",\n",
    "    base_seed=1234,\n",
    ")\n",
    "\n",
    "# --------------------------------------------------\n",
    "# Storage for results\n",
    "# --------------------------------------------------\n",
    "records = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9d7f93ac-e26e-4b71-9bf6-a2060e0dcdb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Training steps: 10000 ===\n",
      "  Run 1/5 (seed=10)\n",
      "Episode finished after 672 steps\n",
      "    → Run reward = 0.73\n",
      "  Run 2/5 (seed=11)\n",
      "Episode finished after 672 steps\n",
      "    → Run reward = 0.41\n",
      "  Run 3/5 (seed=12)\n",
      "Episode finished after 672 steps\n",
      "    → Run reward = 0.65\n",
      "  Run 4/5 (seed=13)\n",
      "Episode finished after 672 steps\n",
      "    → Run reward = 0.76\n",
      "  Run 5/5 (seed=14)\n",
      "Episode finished after 672 steps\n",
      "    → Run reward = 0.50\n",
      "  → mean reward = 0.61 ± 0.13\n",
      "\n",
      "=== Training steps: 20000 ===\n",
      "  Run 1/5 (seed=10)\n",
      "Episode finished after 672 steps\n",
      "    → Run reward = -1.88\n",
      "  Run 2/5 (seed=11)\n",
      "Episode finished after 672 steps\n",
      "    → Run reward = 2.04\n",
      "  Run 3/5 (seed=12)\n",
      "Episode finished after 672 steps\n",
      "    → Run reward = 3.78\n",
      "  Run 4/5 (seed=13)\n",
      "Episode finished after 672 steps\n",
      "    → Run reward = 3.81\n",
      "  Run 5/5 (seed=14)\n",
      "Episode finished after 672 steps\n",
      "    → Run reward = 1.04\n",
      "  → mean reward = 1.76 ± 2.10\n",
      "\n",
      "=== Training steps: 40000 ===\n",
      "  Run 1/5 (seed=10)\n",
      "Episode finished after 672 steps\n",
      "    → Run reward = -1.31\n",
      "  Run 2/5 (seed=11)\n",
      "Episode finished after 672 steps\n",
      "    → Run reward = 8.74\n",
      "  Run 3/5 (seed=12)\n",
      "Episode finished after 672 steps\n",
      "    → Run reward = 9.14\n",
      "  Run 4/5 (seed=13)\n",
      "Episode finished after 672 steps\n",
      "    → Run reward = 5.23\n",
      "  Run 5/5 (seed=14)\n",
      "Episode finished after 672 steps\n",
      "    → Run reward = 11.79\n",
      "  → mean reward = 6.72 ± 4.52\n",
      "\n",
      "=== Training steps: 60000 ===\n",
      "  Run 1/5 (seed=10)\n",
      "Episode finished after 672 steps\n",
      "    → Run reward = 12.01\n",
      "  Run 2/5 (seed=11)\n",
      "Episode finished after 672 steps\n",
      "    → Run reward = 3.67\n",
      "  Run 3/5 (seed=12)\n",
      "Episode finished after 672 steps\n",
      "    → Run reward = 1.21\n",
      "  Run 4/5 (seed=13)\n",
      "Episode finished after 672 steps\n",
      "    → Run reward = 7.64\n",
      "  Run 5/5 (seed=14)\n",
      "Episode finished after 672 steps\n",
      "    → Run reward = 11.45\n",
      "  → mean reward = 7.19 ± 4.24\n",
      "\n",
      "=== Training steps: 80000 ===\n",
      "  Run 1/5 (seed=10)\n",
      "Episode finished after 672 steps\n",
      "    → Run reward = 13.48\n",
      "  Run 2/5 (seed=11)\n",
      "Episode finished after 672 steps\n",
      "    → Run reward = 3.97\n",
      "  Run 3/5 (seed=12)\n",
      "Episode finished after 672 steps\n",
      "    → Run reward = 7.55\n",
      "  Run 4/5 (seed=13)\n",
      "Episode finished after 672 steps\n",
      "    → Run reward = 7.13\n",
      "  Run 5/5 (seed=14)\n",
      "Episode finished after 672 steps\n",
      "    → Run reward = 2.64\n",
      "  → mean reward = 6.95 ± 3.75\n",
      "\n",
      "=== Training steps: 100000 ===\n",
      "  Run 1/5 (seed=10)\n",
      "Episode finished after 672 steps\n",
      "    → Run reward = 6.24\n",
      "  Run 2/5 (seed=11)\n",
      "Episode finished after 672 steps\n",
      "    → Run reward = 4.80\n",
      "  Run 3/5 (seed=12)\n",
      "Episode finished after 672 steps\n",
      "    → Run reward = 5.44\n",
      "  Run 4/5 (seed=13)\n",
      "Episode finished after 672 steps\n",
      "    → Run reward = 3.83\n",
      "  Run 5/5 (seed=14)\n",
      "Episode finished after 672 steps\n",
      "    → Run reward = 10.42\n",
      "  → mean reward = 6.15 ± 2.28\n",
      "\n",
      "=== Summary ===\n",
      "   training_steps      mean       std\n",
      "0           10000  0.611316  0.150125\n",
      "1           20000  1.758557  2.350664\n",
      "2           40000  6.719337  5.058476\n",
      "3           60000  7.193294  4.735968\n",
      "4           80000  6.953572  4.197592\n",
      "5          100000  6.146870  2.547653\n"
     ]
    }
   ],
   "source": [
    "# ==================================================\n",
    "# Main experiment loop\n",
    "# ==================================================\n",
    "for total_steps in training_steps_list:\n",
    "    print(f\"\\n=== Training steps: {total_steps} ===\")\n",
    "\n",
    "    run_rewards = []\n",
    "\n",
    "    for run_id in range(n_runs):\n",
    "        seed = base_seed + run_id\n",
    "        print(f\"  Run {run_id+1}/{n_runs} (seed={seed})\")\n",
    "\n",
    "        # -----------------------------\n",
    "        # Training env\n",
    "        # -----------------------------\n",
    "        train_env_raw = BatteryEnv(\n",
    "            price_series=price_train,\n",
    "            timestamps=ts_train,\n",
    "            dt_hours=dt_hours,\n",
    "            capacity_kWh=50.0,\n",
    "            p_max_kW=10.0,\n",
    "            use_discrete_actions=True,\n",
    "            use_price_forecast=True,\n",
    "            forecast_horizon_hours=forecast_horizon_hours,\n",
    "            episode_days=7.0,\n",
    "            random_start=True,\n",
    "            random_seed=seed,\n",
    "            price_scenario_gen=price_scenario_gen,\n",
    "        )\n",
    "\n",
    "        train_env = Monitor(train_env_raw)\n",
    "\n",
    "        model = DQN(\n",
    "            \"MlpPolicy\",\n",
    "            train_env,\n",
    "            learning_rate=1e-3,\n",
    "            buffer_size=50_000,\n",
    "            learning_starts=1_000,\n",
    "            batch_size=64,\n",
    "            gamma=0.99,\n",
    "            train_freq=4,\n",
    "            target_update_interval=1_000,\n",
    "            exploration_initial_eps=1.0,\n",
    "            exploration_final_eps=0.05,\n",
    "            exploration_fraction=0.3,\n",
    "            verbose=0,\n",
    "            seed=seed,\n",
    "        )\n",
    "\n",
    "        model.learn(total_timesteps=total_steps)\n",
    "\n",
    "        # -----------------------------\n",
    "        # Evaluation env (fixed)\n",
    "        # -----------------------------\n",
    "        eval_env = BatteryEnv(\n",
    "            price_series=price_eval,\n",
    "            timestamps=ts_eval,\n",
    "            dt_hours=dt_hours,\n",
    "            capacity_kWh=50.0,\n",
    "            p_max_kW=10.0,\n",
    "            use_discrete_actions=True,\n",
    "            use_price_forecast=True,\n",
    "            forecast_horizon_hours=forecast_horizon_hours,\n",
    "            episode_days=7.0,\n",
    "            random_start=False,\n",
    "            random_seed=999,\n",
    "            price_scenario_gen=price_scenario_gen,\n",
    "            scenario_id=0,\n",
    "            vary_scenario_per_episode=False,\n",
    "        )\n",
    "\n",
    "        rollout = evaluate_rollout(model=model, env=eval_env)\n",
    "\n",
    "        total_reward = np.sum(rollout[\"reward\"])\n",
    "        run_rewards.append(total_reward)\n",
    "\n",
    "        print(f\"    → Run reward = {total_reward:.2f}\")\n",
    "\n",
    "        records.append({\n",
    "            \"agent\": \"DQN\",\n",
    "            \"training_steps\": total_steps,\n",
    "            \"run_id\": run_id,\n",
    "            \"seed\": seed,\n",
    "            \"total_reward\": total_reward,\n",
    "        })\n",
    "\n",
    "    # -----------------------------\n",
    "    # Summary for this step size\n",
    "    # -----------------------------\n",
    "    mean_r = np.mean(run_rewards)\n",
    "    std_r = np.std(run_rewards)\n",
    "\n",
    "    print(f\"  → mean reward = {mean_r:.2f} ± {std_r:.2f}\")\n",
    "\n",
    "# --------------------------------------------------\n",
    "# Convert to DataFrame\n",
    "# --------------------------------------------------\n",
    "df_results = pd.DataFrame(records)\n",
    "\n",
    "summary = (\n",
    "    df_results\n",
    "    .groupby(\"training_steps\")[\"total_reward\"]\n",
    "    .agg([\"mean\", \"std\"])\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "print(\"\\n=== Summary ===\")\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "87849a16-a8b8-480d-af57-1422c0c2b8c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[save_experiment_records] Saved 30 new rows (total=90) to results/learning_steps_records.csv\n"
     ]
    }
   ],
   "source": [
    "save_records(\n",
    "    records=records,\n",
    "    out_path=\"results/learning_steps_records.csv\",\n",
    "    experiment_id=\"dqn_learning_steps_v1\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0528cc0-f42f-48f3-9899-14a37d7bffa9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (drl_bess)",
   "language": "python",
   "name": "drl_bess"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
